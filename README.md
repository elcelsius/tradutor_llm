# Tradutor de Light Novels (EN ‚Üí PT-BR) ‚Äì Windows-friendly

Pipeline completo para traduzir PDFs/Markdown usando LLMs (Ollama/Gemini), com etapas autom√°ticas de limpeza, desquebrar, tradu√ß√£o, refine e PDF final.

> Para iniciantes: siga a se√ß√£o **Passo a passo r√°pido**. Tudo √© configurado pelo `config.yaml`.

---

## Passo a passo r√°pido
1) Instale depend√™ncias:
```bash
pip install -r requirements.txt
```
2) Ajuste o `config.yaml` (modelos, caminhos, fonte do PDF). Padr√£o: Ollama rodando localmente.
3) Coloque seus PDFs em `data/`.
4) Rode a tradu√ß√£o completa (com refine; PDF √© opcional e s√≥ sai se estiver habilitado):
```bash
python -m tradutor.main traduz --input "data/meu_livro.pdf"
```
5) Sa√≠das em `saida/`:
   - `<slug>_pt.md` (tradu√ß√£o)
   - `<slug>_pt_refinado.md` (refine)
   - `pdf/<slug>_pt_refinado.pdf` (apenas se `pdf_enabled: true` ou flag `--pdf-enabled`)
   - m√©tricas/manifestos para auditoria.

---

## O que o pipeline faz
- **Pr√©-processa** o PDF (limpa lixo b√°sico).
- **Desquebra** linhas (usa LLM configurado em `desquebrar_*`).
- **Traduz** EN ‚Üí PT-BR com contexto leve e gloss√°rio opcional.
- **Cleanup antes do refine** (remove duplicatas/glued, etc).
- **Refina** o PT-BR com guardrails.
- **Gera PDF** com fonte configur√°vel (ReportLab).

---

## Configura√ß√£o (config.yaml)
Principais chaves (padr√µes j√° preenchidos):
- `translate_backend`, `translate_model` (ex.: `gemma3:27b-it-q4_K_M`), `translate_temperature`, `translate_repeat_penalty`, `translate_chunk_chars`, `translate_num_predict`.
- `use_desquebrar` (true/false) e `desquebrar_*` (backend/model/temp/repeat_penalty/chunk/num_predict).
- `refine_backend`, `refine_model` (ex.: `mistral-small3.1:24b-instruct-2503-q4_K_M`), `refine_temperature`, `refine_guardrails`, `cleanup_before_refine` (off/auto/on).
- PDF: `pdf_enabled` (padr√£o false; habilite no config ou com `--pdf-enabled`), `pdf_font.file/size/leading`, `pdf_font_fallbacks`, `pdf_margin`, `pdf_author`, `pdf_language`.
- Caminhos: `data_dir`, `output_dir`.

> O desquebrar usa exatamente o modelo/backend definidos em `config.yaml`; nada hardcoded.

---

## üìò Gloss√°rios
- Gloss√°rios s√£o dados editoriais espec√≠ficos de cada projeto/obra. N√£o versione gloss√°rios reais.
- Exemplo de refer√™ncia: `glossario/glossario_exemplo.json` (15 termos gen√©ricos com campos `term`, `translation`, `type`, `locked`, `notes`, `aliases`).
- Estrutura b√°sica (JSON):
  - `term`: termo de origem.
  - `translation`: tradu√ß√£o fixa.
  - `type`: categoria (ex.: creature, place, magic, title, organization, item, event).
  - `locked`: true/false para fixar a tradu√ß√£o.
  - `notes`: observa√ß√£o opcional.
  - `aliases`: lista opcional de varia√ß√µes do termo.
- Para criar o seu: copie `glossario_exemplo.json`, edite os termos e aponte no CLI com `--manual-glossary <seu_glossario.json>`.
- Todos os `glossario/*.json` s√£o ignorados no Git, exceto `glossario_exemplo.json`.

---

## Comandos principais e flags

### Traduzir PDF ‚Üí PT-BR (com refine e PDF, se habilitado)
```bash
python -m tradutor.main traduz --input "data/meu_livro.pdf"
```
Flags (todas opcionais):
- `--backend {ollama,gemini}` / `--model <nome>`: override de backend/modelo de tradu√ß√£o.
- `--num-predict <int>`: tokens m√°ximos por chunk na tradu√ß√£o.
- `--no-refine`: pula o refine (gera s√≥ `<slug>_pt.md`).
- `--refine-mode {llm,safe}`: `safe` usa desquebrar conservador sem LLM (preserva layout) antes da tradu√ß√£o/refine.
- `--resume`: retoma a partir do manifesto de progresso da tradu√ß√£o.
- `--use-glossary`: injeta gloss√°rio manual (JSON) na tradu√ß√£o.
- `--manual-glossary <path>`: caminho do gloss√°rio manual (default `glossario/glossario_manual.json`).
- `--parallel <n>`: workers paralelos (tradu√ß√£o for√ßa ordem; >1 pode ser limitado).
- `--preprocess-advanced`: limpeza extra antes de traduzir.
- `--cleanup-before-refine {off,auto,on}`: for√ßa/auto/desliga cleanup antes do refine.
- `--use-desquebrar` / `--no-use-desquebrar`: ativa/desativa desquebrar pr√©-tradu√ß√£o (default vem do config).
- `--desquebrar-backend/model/temperature/repeat-penalty/chunk-chars/num-predict`: overrides espec√≠ficos do desquebrar.
- `--debug`: salva artefatos intermedi√°rios (`*_raw_extracted.md`, `*_preprocessed.md`, `*_raw_desquebrado.md`).
- `--debug-chunks`: JSONL detalhado por chunk.
- `--pdf-enabled` / `--no-pdf-enabled`: liga/desliga PDF autom√°tico ap√≥s refine (se refine estiver ativo).
- `--request-timeout <s>`: timeout por chamada de modelo.

### Refine separado em um Markdown PT-BR
```bash
python -m tradutor.main refina --input "saida/meu_livro_pt.md"
```
Flags:
- `--backend {ollama,gemini}` / `--model <nome>`: override de refine.
- `--num-predict <int>`: tokens m√°ximos por chunk no refine.
- `--refine-mode {llm,safe}`: `safe` apenas troca o desquebrar pelo modo conservador (sem LLM); refine segue normal.
- `--resume`: retoma a partir do manifesto de refine.
- `--normalize-paragraphs`: normaliza par√°grafos antes de refinar.
- `--use-glossary`: ativa gloss√°rio manual/din√¢mico.
- `--manual-glossary <path>` / `--dynamic-glossary <path>` / `--auto-glossary-dir <dir>`: fontes de gloss√°rio.
- `--debug-refine`: salva debug dos primeiros chunks de refine.
- `--parallel <n>`: workers paralelos (ordem preservada na montagem).
- `--preprocess-advanced`: limpeza extra antes do refine.
- `--cleanup-before-refine {off,auto,on}`: modo de cleanup determin√≠stico.
- `--debug-chunks`: JSONL detalhado por chunk.
- Editor opcional p√≥s-refine: `--editor-lite`, `--editor-consistency`, `--editor-voice`, `--editor-strict`, `--editor-report` (gera `editor_report.json`).
- `--request-timeout <s>`: timeout por chamada.

### Gerar PDF a partir de um Markdown existente
```bash
python -m tradutor.main pdf --input "saida/meu_livro_pt_refinado.md"
```
Usa as configs de fonte/margem do `config.yaml`. Sem flags adicionais al√©m de `--debug` (para logs verbosos).

### Usar desquebrar direto em um arquivo
```bash
python desquebrar.py --input "arquivo.md" --output "arquivo_desquebrado.md" --config config.yaml
```
Flags: `--config` (opcional), `--debug` (logs). As demais configs v√™m do `config.yaml`.
---

## Estrutura de pastas
```
tradutor/
  main.py             # CLI principal (traduz/refina/pdf)
  translate.py        # pipeline de tradu√ß√£o em chunks
  desquebrar.py       # fun√ß√£o de desquebrar usada no pipeline
  refine.py           # refine e cleanup determin√≠stico
  pdf.py              # conversor Markdown ‚Üí PDF (ReportLab)
  config.py           # carrega/mescla config.yaml
  cleanup.py          # heur√≠sticas determin√≠sticas (dedupe, prefixos)
  preprocess.py       # pr√©-processo de PDFs e chunking seguro
  advanced_preprocess.py # limpeza opcional extra
  sanitizer.py        # sanitiza√ß√£o de sa√≠da LLM
  anti_hallucination.py # filtros AAA anti-alucina√ß√£o/repeti√ß√£o
  cache_utils.py      # cache/hash por chunk, resume
  glossary_utils.py   # carga/merge/gloss√°rio din√¢mico
  pdf_reader.py       # extra√ß√£o de texto de PDF (fitz)
  pdf_export.py       # exportador PDF legado (ReportLab)
  postprocess.py      # ajustes finais em PT-BR
  structure_normalizer.py # normaliza t√≠tulos/cabe√ßalhos
  editor.py           # modos editor opcionais (lite/consistency/voice/strict)
  llm_backend.py      # cliente LLM (Ollama/Gemini)
  benchmark.py        # benchmark BLEU/chrF
  bench_llms.py       # benchmark r√°pido de tradu√ß√£o
  bench_refine_llms.py# benchmark r√°pido de refine
  VERSION             # vers√£o interna do pipeline

data/                 # PDFs de entrada
saida/
  cache_*             # caches de tradu√ß√£o/refine/desquebrar
  pdf/                # PDFs finais gerados
  *_pt.md             # tradu√ß√£o
  *_pt_refinado.md    # refine
  *metrics.json       # m√©tricas de cada etapa
  *progress.json      # manifestos de progresso
  glossario_dinamico.json # se gloss√°rio din√¢mico estiver ativo

glossario/            # gloss√°rios manuais por volume
benchmark/            # insumos para benchmarks
tests/                # testes (smoke e unit√°rios)
config.yaml           # configura√ß√£o central (modelos, fontes, caminhos)
config.example.yaml   # exemplo de configura√ß√£o comentado
desquebrar.py         # wrapper CLI para desquebrar direto
tradutor.py / refinador.py # wrappers legados (chamam main)
```

---

## Sa√≠das e auditoria
- Tradu√ß√£o: `saida/<slug>_pt.md` + m√©tricas `*_translate_metrics.json` + `report.json`.
- Refine: `saida/<slug>_pt_refinado.md` + `*_refine_metrics.json`.
- Desquebrar (se debug): `*_raw_extracted.md`, `*_raw_desquebrado.md`, m√©tricas `*_desquebrar_metrics.json`.
- PDF: `saida/pdf/<slug>_pt_refinado.pdf` (quando `pdf_enabled: true`).
- Manifestos de progresso: `*_progress.json` (trad/refine).

---

## Requisitos
- Windows 11 (priorit√°rio), Python 3.10+.
- Depend√™ncias: `pip install -r requirements.txt`.
- Backend: Ollama (padr√£o) ou Gemini (`GEMINI_API_KEY` no ambiente).

---

## Dicas r√°pidas
- Modelos sugeridos (Ollama):
  - Tradu√ß√£o: `gemma3:27b-it-q4_K_M`
  - Desquebrar: use o mesmo ou outro em `config.yaml`.
  - Refine: `mistral-small3.1:24b-instruct-2503-q4_K_M`
- Para evitar truncamentos: mantenha `translate_chunk_chars` em ~2400.
- Se fonte do PDF n√£o existir, ajuste `pdf_font.file` ou use um fallback v√°lido (ex.: `C:/Windows/Fonts/Arial.ttf`).

---

## Testes
- Smoke: `pytest -q` (usa Fakes/stubs; n√£o chama LLM real).
- Benchmarks opcionais em `benchmark/` e comandos `bench_llms`/`bench_refine_llms`.
